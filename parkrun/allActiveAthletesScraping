#using an imput file - a list of all parkruns, get a list of all athletes
#listed on each latest result page

import requests
from bs4 import BeautifulSoup

with open('c:\\temp\parkrunlist.txt') as file: #a containing each parkrun name on each line
    lines = file.readlines() #read the file into a variable
    lines = [line.rstrip() for line in lines] #put the variable into a list


headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}

links = []

with open('c:\\temp\\runners.txt','w') as f: #blank the output file
    f.write('')

for venue in lines:
    print (venue) #so we can watch progression

    url = "https://www.parkrun.org.uk/"+ str(venue) +"/results/latestresults/"

    html = requests.get(url,headers=headers).content

    response = requests.get(url,headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    table = soup.find('table')

    
    for tr in table.findAll("tr"): #for each row
        trs = tr.findAll("a") #find the links
        for each in trs:
            if (str(each.get('href')).find('parkrunner')) != -1: #check if the link is an athlete url
                link =     str(each.get('href'))
                with open('c:\\temp\\runners.txt','a') as f:
                    f.write(link.replace('/'+venue,'')  +'/all/\n') #write it to a file

#output may not be unique as latest event may not be this week (cancellations) - dedupe file in next step 
#purpose is to get as many active valid runners urls as poss from all over the country so its fine if dont restirct only to this week
